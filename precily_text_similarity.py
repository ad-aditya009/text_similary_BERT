# -*- coding: utf-8 -*-
"""Precily_Text_Similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gNIu7RxwPAdo6PH-aZfdtT9V2-4usH-g

'''
      Text similarity using BERT pretrained models. Three latest bert models 
      have been used and text similarity scores are obtained, lastly taking 
      their average as result of the text similarity task.

       score1 : BERT model 'paraphrase-MiniLM-L3-v2'
       score2 : BERT model 'all-distilroberta-v1'
       score3 : BERT model 'multi-qa-distilbert-cos-v1'
       avg_simi_score : average of the similarity scores from the three models.
       similarity measure used - cosine similarity
       
'''
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Import libraries
import pandas as pd
import numpy as np

# Read csv to dataframe
cols =['text1', 'text2']
path = "/content/drive/MyDrive/Precily_Text_Similarity.csv"
df = pd.read_csv(path, names = cols, skiprows=1)
df.head()

# Stop word removal
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.
df['text1_nstop'] = df['text1'].apply(lambda x: ' '.join([word for word in \
                              x.split() if word not in (stop)]))
df['text2_nstop'] = df['text2'].apply(lambda x: ' '.join([word for word in \
                              x.split() if word not in (stop)]))
df.head()

# Tokenizing data
nltk.download('punkt')
df["tok_text1"] = df.apply(lambda row: nltk.word_tokenize \
                             (row["text1_nstop"]), axis=1)
df["tok_text2"] = df.apply(lambda row: nltk.word_tokenize \
                             (row["text2_nstop"]), axis=1)
# Convert obtained lists result from tokenizer to string data points
df['tok_text1_str'] = [' '.join(map(str, l)) for l in \
                       df['tok_text1']]
df['tok_text2_str'] = [' '.join(map(str, l)) for l in \
                       df['tok_text2']]
df.head()

# Lemmatized data columns
nltk.download('wordnet')
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
df['text1_lemmatized'] = df.tok_text1_str.apply(lemmatize_text)
df['text2_lemmatized'] = df.tok_text2_str.apply(lemmatize_text)
# Convert obtained lists result from lemmatizer to string data points
df['text1_lemm_str'] = [' '.join(map(str, l)) for l in \
                         df['text1_lemmatized']]
df['text2_lemm_str'] = [' '.join(map(str, l)) for l in \
                         df['text2_lemmatized']]
# Convert lemmatized data to lower case
df['text1_lemm_str'] = df['text1_lemm_str'].str.lower() 
df['text2_lemm_str'] = df['text2_lemm_str'].str.lower()
df.head()

# Transformers package by Hugging Face
!pip install transformers

# Framework for sentence, text, image embedding by SBERT
!pip install sentence-transformers

# Import BERT models using SentenceTransformer
from sentence_transformers import SentenceTransformer, util
import numpy as np
model1 = SentenceTransformer('paraphrase-MiniLM-L3-v2')
model2 = SentenceTransformer('all-distilroberta-v1')
model3 = SentenceTransformer('multi-qa-distilbert-cos-v1')

# Create and intialize columns for similarity scores
score = [0.00]*3000
pd.Series(score)
df['score1']= score
df['score2']= score
df['score3']= score

# A sentence/text to tensor size by each models
a = df.at[0,'text1_lemm_str']
embda1 = model1.encode(a, convert_to_tensor=True)
embda2 = model2.encode(a, convert_to_tensor=True)
embda3 = model3.encode(a, convert_to_tensor=True)
m1_shape = embda1.shape
m2_shape = embda2.shape
m3_shape = embda3.shape
print("BERT model1 'paraphrase-MiniLM-L3-v2', tensor size    :", m1_shape)
print("BERT model2 'all-distilroberta-v1', tensor size       :", m2_shape)
print("BERT model3 'multi-qa-distilbert-cos-v1', tensor size :", m3_shape)

df.head()

# Text similarity scores obtained using 'paraphrase-MiniLM-L3-v2' BERT model
import time
st = time.time()
for i in range(0,3000):
  s1= df.at[i, 'text1_lemm_str']
  s2= df.at[i, 'text2_lemm_str']
  embd1 = model1.encode(s1, convert_to_tensor=True)
  embd2 = model1.encode(s2, convert_to_tensor=True)
  cosine_scores = util.pytorch_cos_sim(embd1, embd2)
  df.at[i,'score1'] = cosine_scores.item()
et = time.time()
df.to_csv("/content/drive/MyDrive/similarity_bert_m1.csv")
elapsed_time = et - st
print('Execution time:', elapsed_time, 'seconds')

# Text similarity scores obtained using BERT model 'all-distilroberta-v1'
for i in range(0,3000):
  s1= df.at[i, 'text1_lemm_str']
  s2= df.at[i, 'text2_lemm_str']
  embd1 = model2.encode(s1, convert_to_tensor=True)
  embd2 = model2.encode(s2, convert_to_tensor=True)
  cosine_scores = util.pytorch_cos_sim(embd1, embd2)
  df.at[i,'score2'] = cosine_scores.item()
df.to_csv("/content/drive/MyDrive/similarity_bert_m2.csv")
et = time.time()

# Text similarity scores obtained using BERT model  'multi-qa-distilbert-cos-v1'
for i in range(0,3000):
  s1= df.at[i, 'text1_lemm_str']
  s2= df.at[i, 'text2_lemm_str']
  embd1 = model3.encode(s1, convert_to_tensor=True)
  embd2 = model3.encode(s2, convert_to_tensor=True)
  cosine_scores = util.pytorch_cos_sim(embd1, embd2)
  df.at[i,'score3'] = cosine_scores.item()

# Taking average of the text similarity scores
df['avg_similarity_score'] = df.iloc[:, 12:15].mean(axis=1)
df.head()

# Taking results in seperate dataframe
df_result = df[['text1', 'text2', 'score1', 'score2', 'score3', \
               'avg_similarity_score']]
df_result.rename(columns = {'avg_similarity_score':'avg_simi_score'}, \
                 inplace = True)
df_result.head(10)

df_result.shape

# Plot of text similarity scores
import matplotlib.pyplot as mp

# 25 rows(1400 to 1424) taken for the plot
df_result.iloc[1400:1425].plot(y=['score1', 'score2', 'score3', \
                                  'avg_simi_score'],
        kind="line", figsize=(10, 10))
print("      score1 : BERT model 'paraphrase-MiniLM-L3-v2'")
print("      score2 : BERT model 'all-distilroberta-v1'")
print("      score3 : BERT model 'multi-qa-distilbert-cos-v1'")
